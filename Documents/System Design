System Design
The system will be designed with a modular, scalable architecture that supports the functional needs and non-functional requirements described. Here we outline the high-level architecture and key components, including how AI and scaling are handled. Architecture Overview: Windrush will follow a standard web application architecture with a separation between the front-end client, back-end services (business logic and APIs), and the database (storage layer). The deployment will likely be cloud-based (e.g. on AWS or Azure) to leverage easy scaling and managed services. At a high level, the system comprises:
Client (Front-End): This is the web application that runs in users’ browsers (and possibly a mobile app in the future). The front-end will be a dynamic single-page application (SPA) built with a framework like React. It will be responsible for the user interface – showing pages for job search, profile, etc., and making asynchronous calls to the back-end APIs to fetch or send data. The front-end will handle routing (with different views for job seeker vs employer functionalities) and store some state (like the user’s login token, current search filters, etc.). Using a modern front-end ensures a snappy, app-like experience for users. It will communicate with the server via JSON (RESTful APIs) or GraphQL queries, typically over HTTPS. We will also make the front-end responsive so it works on desktop or mobile browsers. For certain features like real-time notifications or chat, the front-end might use WebSockets or long-polling to receive updates from the server without needing a page refresh.
Back-End (Server Side): The back-end consists of the application logic and the API that the front-end (and any other clients) interact with. We are likely to implement a REST API that exposes endpoints for various actions (e.g. GET /jobs, POST /apply, etc.). This back-end can be structured as a monolithic application or split into microservices. Given the need for scalability and the distinct domains (jobs, users, applications, AI, etc.), a modular monolith initially, evolving into microservices as needed, is a prudent approach. Key components/services might include:
User Service: Handles authentication (login, registration), profile management, password resets, and user-related operations. It enforces security (password hashing, JWT token issuance). It also manages user preferences and premium subscription status.
Job Service: Manages everything related to job listings and company profiles. This service provides endpoints to search jobs (possibly integrating with a search engine), get job details, create or update a job (for admins/employers), etc. It contains the business logic for filtering only visa-sponsor jobs. It also might interface with an external service or library for full-text search if needed (for example, using Elasticsearch to handle keyword searches efficiently).
Application Service: Manages job applications and speculative applications. Endpoints here handle “apply to job” and “send speculative application” actions. This service enforces rules like limiting number of applications for free users, and it triggers notifications to employers. It also includes logic for application tracking – updating status, etc. When an application is submitted, this service would create an application record in the database, store the attached documents (or reference to them), and perhaps send an email confirmation.
AI Recommendation Service: This could be a separate microservice or a module within the back-end that generates job recommendations for users. It might run heavier computation – for example, using a machine learning model to rank jobs for a user. For scalability, we might implement this as an offline batch process: periodically compute top N recommendations for each user and store them, so that when the user logs in, the front-end just calls an API to get the pre-computed recommendations. Alternatively, a real-time approach could call the AI model on-demand. Depending on complexity, this might be separated to allow using Python ML libraries. We could also initially use a simpler rules engine integrated in the main app, then later transition to a proper ML service.
Notification/Messaging Service: A component responsible for sending out notifications (email, and pushing in-app notifications). This could be done via a job queue system – for example, when certain events happen (new application, status change), other services will enqueue a notification event which this service picks up and processes (sending an email via an SMTP service or using a service like SendGrid, and saving an in-app notification to the DB). For real-time messaging, we might use a WebSocket gateway that pushes messages to connected clients.
Admin/Analytics Service: This covers any internal admin panel functionality, such as viewing metrics or managing content. It might gather data from other services or directly from the database (with appropriate privileges).
These services can either run as separate processes/containers or as part of one application process with clear separation. In early development, implementing a single back-end application (e.g. a Node.js Express app or a Django app) with modules for each area might be simplest, but we will maintain a clean architecture so that we can split services out as load increases. The back-end will also integrate with third-party APIs as needed – for example, an email service API, possibly a payment API (for premium subscriptions), or a cloud storage SDK for file uploads.
Database (Storage): We plan to use a relational database (SQL) as the primary data store, given the structured nature of the data (users, jobs, applications have clear relations). PostgreSQL is a strong candidate for its reliability and features (including full-text search capability that might help job search). The database will contain tables for users, companies, jobs, applications, etc. (see next section for schema). Using a relational DB ensures ACID compliance – important for things like not losing an application record or maintaining consistency (e.g. if a job is deleted, associated applications should also be handled). We might also use additional specialized storage:
A search index: If job searching by keywords and filters becomes too slow on SQL (especially with many listings), we could use Elasticsearch or Apache Solr to index job postings and enable fast text queries and filtering. The Job Service would then query the search index rather than doing complex SQL queries.
A cache: To reduce DB load, a caching layer (like Redis) might be used for frequently accessed data (e.g. caching the sponsor company list, or caching popular job search results). Redis can also be used for sessions or token blacklists if needed.
File storage: User-uploaded documents (CVs, cover letters) and possibly images (profile pictures, company logos) will not be stored directly in the SQL database (to keep it lean). Instead, we’ll use cloud object storage (like AWS S3) to store files, and keep only the file URL or path in the database. This allows serving files efficiently via CDN and keeps DB operations fast. We will generate unique file names and paths for each upload and ensure access control (e.g. signed URLs if privacy is needed for CVs).
Scalability and Infrastructure: To meet the requirement of handling 50k users and beyond, the system will be deployed in a scalable cloud environment:
We will use a Load Balancer in front of multiple application server instances. Users connect to a single URL (e.g. windrush.co.uk) which is served by the load balancer, and it distributes incoming requests to whichever app instance is least busy

. This allows us to run, say, 5 instances of the back-end and easily increase to 10 or 20 as traffic grows. The app servers will be stateless (all persistent data in the DB or shared cache) so any instance can serve any request. This stateless design is crucial for horizontal scaling.
For the database, initially a single instance might suffice. As usage grows, we can scale the DB vertically (more CPU/RAM) and horizontally by adding read replicas (for load-heavy read operations like browsing jobs) or partitioning data if necessary. In extreme scale scenarios, a sharded database or distributed SQL might be needed, but for 50k users a master with replicas is likely enough. We’ll also consider using a managed database service that can scale storage and handle replication easily (AWS RDS or Aurora, for example).
The application and database will be deployed in multiple availability zones (data centers) so that even if one goes down, another can take over, improving reliability.
We will implement auto-scaling rules – e.g. if CPU usage on servers stays high for some minutes, automatically launch new server instances. Conversely, scale down in low traffic periods to save cost. This elasticity ensures we can handle spikes (maybe around graduation season when many students are job hunting) without over-provisioning for the rest of the time

.
Content Delivery Network (CDN): All static assets (images, CSS/JS files, and potentially job images or company logos) will be served via a CDN for faster global access and less load on our servers. This also helps deliver content quickly to users in the UK and abroad.
We will isolate environments (development, staging, production) and likely use containerization (Docker) for deployment to ensure consistency. Using a container orchestration (like Kubernetes or AWS ECS) can simplify scaling and management, though initially a simpler PaaS or VM-based deployment might suffice until scale demands orchestration.
Logging and monitoring infrastructure will be in place (services like CloudWatch, ELK stack, or DataDog) to monitor performance and errors in real time, so we can proactively address any bottlenecks or failures.
AI/ML Integration: The AI job suggestion feature requires some additional design considerations:
We’ll likely maintain a separate ML model training pipeline. For example, we might use Python with scikit-learn or TensorFlow to train a recommendation model (perhaps a matrix factorization or a neural network that takes user and job features). This could be trained on data like “users with profile X applied to jobs like Y”. Initially, with limited data, a simpler content-based filtering (matching user skills to job requirements) can be coded directly without a complex model. But as data grows, we can shift to data-driven ML.
If using a heavy ML model, we may host it as a service (e.g. a Python microservice that our main Node/JavaScript backend calls to get recommendations). This microservice could have an API like /recommend?userId=123 and it returns a list of job IDs with scores. Alternatively, we might use a cloud ML service (like AWS Sagemaker or Google AI Platform) to host and serve the model.
The recommendation computations might be done offline on a schedule (e.g. recompute top recommendations daily for each active user) to avoid latency when the user is online. These precomputed recommendations can be stored in a table or cache.
We will also incorporate feedback loops: for instance, if a user ignores a certain type of recommendation repeatedly, the system should adjust (this can be done by simple rules or re-training the model periodically).
Data required for AI (like job feature vectors, user skill vectors) will be extracted from the main database. We might have a separate analytics database or data warehouse where we dump relevant data for ML and analysis so that running ML jobs doesn’t strain the production DB.
Handling File Uploads: When a user uploads a CV or cover letter, the file goes from the browser to the back-end (we’ll likely implement a secure direct upload to S3 to avoid routing large files through our server unnecessarily). The system will generate a unique file key (maybe including the user ID and job ID) and store the file in an S3 bucket. The back-end then saves the reference (URL or key) in the database tied to the application or user. We’ll enforce file type and size limits (only PDFs or DOCX perhaps, max a few MB) to ensure storage is manageable and to prevent abuse. Virus scanning can be done asynchronously – e.g. using an AWS Lambda trigger on new S3 objects to run a scan, or using a service. If a virus is detected, we’d flag/remove the file and notify the user. Security Design: We will use JWT (JSON Web Tokens) or session tokens to maintain user sessions after login. If JWT, the token is stored in the client (probably HTTP-only cookies for safety or local storage if needed) and sent with each request to authenticate. The back-end verifies the token signature and decodes the user info. We’ll also have refresh tokens or short expiry on tokens for security. The API will authenticate every request and authorize based on user roles (for example, an employer cannot call an endpoint to update someone else’s job post, etc.). Sensitive operations (like password change, or anything dealing with premium payments) will require re-auth or other checks to prevent CSRF or misuse. We’ll also guard against brute force (rate-limit login attempts) and use captcha as needed for bots. Compliance in Design: We incorporate GDPR compliance by design: e.g., logging of consent timestamps, providing endpoints for data export/delete so we can fulfill those requests easily. The database design will allow marking records as deleted (or truly deleting them) when a user invokes the right to be forgotten. We’ll also separate personally identifiable information (PII) from public data where possible. For example, a user’s profile might have an internal user_id that links applications; if they delete their account, we can remove their name/email but keep some aggregate stats in anonymous form. The system will be designed to make such compliance tasks straightforward. Diagram Description: (If we had an architecture diagram, it would show: Users (Job Seekers, Employers) via web/mobile -> HTTP requests to Load Balancer -> multiple Application Server instances -> a central Database, plus connections to external services like an Email SMTP, Cloud Storage for files, and an AI Service. The Application Server is internally composed of modules or microservices for user, job, application, etc. The AI service might also access the database or a separate data store. The load balancer ensures no single server is overwhelmed, and scaling out is as easy as adding more servers behind it. The database might have a replica for read queries. We’d also show the CDN serving static files to the user.) In summary, the system design is distributed and robust: it uses a layered approach where each component (front-end, back-end, DB) can scale and be managed independently. By using proven patterns like stateless servers with a load balancer and separating concerns into services, Windrush’s architecture will support both the current requirements and future growth. Once the right architecture is in place, scaling to high user counts is mostly about allocating more resources
blog.xojo.com
 – the design ensures we can do that without redesigning the whole system.